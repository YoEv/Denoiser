[2023-12-07 15:58:20,140][__main__][INFO] - For logs, checkpoints and samples check /Volumes/Castile/HackerProj/denoiser
[2023-12-07 15:58:25,172][denoiser.solver][INFO] - ----------------------------------------------------------------------
[2023-12-07 15:58:25,172][denoiser.solver][INFO] - Training...
[2023-12-07 16:01:49,832][__main__][ERROR] - Some error happened
Traceback (most recent call last):
  File "/Volumes/Castile/HackerProj/denoiser/train.py", line 123, in main
    _main(args)
  File "/Volumes/Castile/HackerProj/denoiser/train.py", line 116, in _main
    run(args)
  File "/Volumes/Castile/HackerProj/denoiser/train.py", line 97, in run
    solver.train()
  File "/Volumes/Castile/HackerProj/denoiser/denoiser/solver.py", line 140, in train
    train_loss = self._run_one_epoch(epoch)  #################
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Volumes/Castile/HackerProj/denoiser/denoiser/solver.py", line 245, in _run_one_epoch
    loss.backward()
  File "/Volumes/Toys/Factory/anaconda3/envs/dnoise/lib/python3.11/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/Volumes/Toys/Factory/anaconda3/envs/dnoise/lib/python3.11/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [20, 128, 34397]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
